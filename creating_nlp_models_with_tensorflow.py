# -*- coding: utf-8 -*-
"""Creating NLP Models with TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tDVkwUmRK4qHb4O3yiK4ZojUeGCxcsAk

##**Creating NLP Models with TensorFlow**

**_import pandas as pd_** used to import pandas modules into the Ipython environment.
The pandas module provides various functions and classes for working with data in **tabular form**.
"""

import pandas as pd

df = pd.read_csv('jobs_in_data.csv')

"""This code functions to read a **_CSV_** file containing job data and save it into a DataFrame in **Python**.

1. df = ... =_This section stores the DataFrame generated by the pd.read_csv() function into a variable named **df**_
2. pd.read_csv() =_fungsi dari library Pandas yang digunakan untuk membaca file **CSV**._
"""

df.head()

"""_**df.head()**_ used to display the top rows of a DataFrame. By default, _**df.head()**_ will display the top 5 rows."""

df.shape

category = pd.get_dummies(df.company_size)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='company_size')
df_baru

"""The code performs a **one-hot encoding** process on the categorical column _"company_size"_ in a pandas DataFrame."""

df.head()

"""_This line of code is used to separate data into two parts, namely input data and label data._"""

jobs = df_baru['job_title'].values
label = df_baru[['M', 'L','S' ]].values

from sklearn.model_selection import train_test_split
jobs_latih, jobs_test, label_latih, label_test = train_test_split(jobs, label, test_size = 0.2 )

"""This code is used to divide the dataset into two parts, namely:

**Training Data** (_Training Set_): Used to train machine learning models.

**Test Data** (_Test Set_): Used to evaluate the performance of the model after training.

"""

from tensorflow.keras.preprocessing.text import Tokenizer         #Mengimpor kelas Tokenizer untuk proses tokenisasi
from tensorflow.keras.preprocessing.sequence import pad_sequences #Mengimpor fungsi pad_sequences untuk menyamakan panjang urutan.

tokenizer = Tokenizer(num_words=5000, oov_token='-')              #Membuat objek Tokenizer dengan batasan 5000 kata yang paling sering muncul.
                                                                  #Kata-kata di luar 5000 kata tersebut akan digantikan dengan token '- ' (oov_token).

tokenizer.fit_on_texts(jobs_latih)                                #Menganalisis teks pada dataset 'jobs_latih' untuk membangun vocabulary (kamus kata).
                                                                  #Menentukan 5000 kata paling sering dan nomor indeksnya

tokenizer.fit_on_texts(jobs_test)                                 #Menganalisis teks pada 'jobs_test' untuk menyesuaikan vocabulary.
                                                                  #Pastikan vocabulary konsisten untuk latih dan test.

sekuens_latih = tokenizer.texts_to_sequences(jobs_latih)          #Mengubah setiap teks pada 'jobs_latih' menjadi urutan angka berdasarkan indeks kata di vocabulary.
sekuens_test = tokenizer.texts_to_sequences(jobs_test)            #Mengubah teks pada 'jobs_test' menjadi urutan angka dengan cara yang sama.

padded_latih = pad_sequences(sekuens_latih)                       #Menyamakan panjang urutan pada 'sekuens_latih' dengan menambahkan nilai 0 di belakangnya.
                                                                  #Diperlukan agar model dapat menerima input dengan dimensi yang sama.

padded_test = pad_sequences(sekuens_test)                         #Menyamakan panjang urutan pada 'sekuens_test'

"""This code functions to convert text into a numerical representation so that it can be processed by a text-based machine learning model."""

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim = 5000, output_dim=16),     #input_dim = 5000: Menandakan jumlah kata unik yang mungkin ada dalam dataset.
                                                                    #output_dim = 16: Menentukan dimensi vektor embedding yang dihasilkan.

    tf.keras.layers.LSTM(64),                                       #Memproses urutan vektor embedding secara bertahap untuk menangkap informasi kontekstual dalam teks.

    tf.keras.layers.Dense(128, activation='relu'),                  #Melakukan klasifikasi berdasarkan informasi yang diekstraksi oleh LSTM.
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

"""This code is a TensorFlow model design for classifying text with 3 output classes"""

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

"""Bagian kode tersebut digunakan untuk mengkompilasi model dan menampilkan ringkasannya.

Kompilasi model adalah proses menentukan fungsi kerugian, pengoptimal, dan metrik yang akan digunakan untuk pelatihan dan evaluasi model.
"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy') > 0.90):
      print('\nakurasi telah mencapai 90%')
      self.model.stop_training = True

callbacks = myCallback()

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

"""Kode tersebut menentukan sebuah callback bernama **myCallback** yang berfungsi untuk menghentikan proses training model secara otomatis ketika akurasi validasi model mencapai 90% atau lebih."""

import matplotlib.pyplot as plt
plt.figure(figsize=(8,5))
plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label='validation_accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim(ymin=0, ymax=1)
plt.show()

plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='validation_loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.ylim(ymin=0)
plt.show()